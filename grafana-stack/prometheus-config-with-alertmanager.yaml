apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-config
  namespace: grafana-stack
data:
  prometheus.yml: |
    global:
      scrape_interval: 15s
      evaluation_interval: 15s
      external_labels:
        cluster: 'grafana-observability-cluster'
        environment: 'development'

    # AlertManager 配置
    alerting:
      alertmanagers:
        - static_configs:
            - targets:
              - alertmanager.grafana-stack.svc.cluster.local:9093
          timeout: 10s
          api_version: v2

    # Remote write to Mimir for long-term storage
    remote_write:
      - url: http://mimir.grafana-stack.svc.cluster.local:8080/api/v1/push
        queue_config:
          max_samples_per_send: 1000
          max_shards: 200
          capacity: 2500

    # Rule files
    rule_files:
      - "rules/*.yml"

    # Scrape configurations
    scrape_configs:
      # Prometheus自身监控
      - job_name: 'prometheus'
        static_configs:
          - targets: ['localhost:9090']

      # AlertManager 监控
      - job_name: 'alertmanager'
        static_configs:
          - targets: ['alertmanager.grafana-stack.svc.cluster.local:9093']

      # Kubernetes API server
      - job_name: 'kubernetes-apiservers'
        kubernetes_sd_configs:
        - role: endpoints
        scheme: https
        tls_config:
          ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
        bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
        relabel_configs:
        - source_labels: [__meta_kubernetes_namespace, __meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name]
          action: keep
          regex: default;kubernetes;https

      # Kubernetes nodes
      - job_name: 'kubernetes-nodes'
        kubernetes_sd_configs:
        - role: node
        scheme: https
        tls_config:
          ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
        bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
        relabel_configs:
        - action: labelmap
          regex: __meta_kubernetes_node_label_(.+)
        - target_label: __address__
          replacement: kubernetes.default.svc:443
        - source_labels: [__meta_kubernetes_node_name]
          regex: (.+)
          target_label: __metrics_path__
          replacement: /api/v1/nodes/${1}/proxy/metrics

      # Kubernetes pods
      - job_name: 'kubernetes-pods'
        kubernetes_sd_configs:
        - role: pod
        relabel_configs:
        - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
          action: keep
          regex: true
        - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
          action: replace
          target_label: __metrics_path__
          regex: (.+)
        - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
          action: replace
          regex: ([^:]+)(?::\d+)?;(\d+)
          replacement: $1:$2
          target_label: __address__
        - action: labelmap
          regex: __meta_kubernetes_pod_label_(.+)
        - source_labels: [__meta_kubernetes_namespace]
          action: replace
          target_label: kubernetes_namespace
        - source_labels: [__meta_kubernetes_pod_name]
          action: replace
          target_label: kubernetes_pod_name

      # Kubernetes services
      - job_name: 'kubernetes-services'
        kubernetes_sd_configs:
        - role: endpoints
        relabel_configs:
        - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape]
          action: keep
          regex: true
        - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scheme]
          action: replace
          target_label: __scheme__
          regex: (https?)
        - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_path]
          action: replace
          target_label: __metrics_path__
          regex: (.+)
        - source_labels: [__address__, __meta_kubernetes_service_annotation_prometheus_io_port]
          action: replace
          target_label: __address__
          regex: ([^:]+)(?::\d+)?;(\d+)
          replacement: $1:$2
        - action: labelmap
          regex: __meta_kubernetes_service_label_(.+)
        - source_labels: [__meta_kubernetes_namespace]
          action: replace
          target_label: kubernetes_namespace
        - source_labels: [__meta_kubernetes_service_name]
          action: replace
          target_label: kubernetes_name

      # Grafana Stack组件监控
      - job_name: 'grafana-stack'
        static_configs:
          - targets: 
            - 'grafana.grafana-stack.svc.cluster.local:3000'
            - 'loki.grafana-stack.svc.cluster.local:3100'
            - 'mimir.grafana-stack.svc.cluster.local:8080'
            - 'tempo.grafana-stack.svc.cluster.local:3200'
            - 'alertmanager.grafana-stack.svc.cluster.local:9093'
        metrics_path: /metrics

  # 扩展的告警规则
  rules.yml: |
    groups:
    - name: kubernetes.rules
      rules:
      - alert: KubernetesPodCrashLooping
        expr: rate(kube_pod_container_status_restarts_total[5m]) > 0
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: "Pod {{ $labels.pod }} is crash looping"
          description: "Pod {{ $labels.pod }} in namespace {{ $labels.namespace }} is restarting frequently"

      - alert: KubernetesPodNotReady
        expr: kube_pod_status_ready{condition="false"} == 1
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Pod {{ $labels.pod }} not ready"
          description: "Pod {{ $labels.pod }} in namespace {{ $labels.namespace }} has been not ready for more than 5 minutes"

      - alert: KubernetesNodeNotReady
        expr: kube_node_status_condition{condition="Ready",status="true"} == 0
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "Node {{ $labels.node }} is not ready"
          description: "Kubernetes node {{ $labels.node }} has been not ready for more than 5 minutes"

      - alert: KubernetesHighCPUUsage
        expr: (100 - (avg by (instance) (rate(node_cpu_seconds_total{mode="idle"}[5m])) * 100)) > 80
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High CPU usage on {{ $labels.instance }}"
          description: "CPU usage is above 80% on {{ $labels.instance }}"

      - alert: KubernetesHighMemoryUsage
        expr: (1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100 > 80
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High memory usage on {{ $labels.instance }}"
          description: "Memory usage is above 80% on {{ $labels.instance }}"

    - name: grafana-stack.rules
      rules:
      - alert: GrafanaDown
        expr: up{job="grafana-stack", instance=~".*grafana.*"} == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "Grafana is down"
          description: "Grafana instance is not responding"

      - alert: MimirDown
        expr: up{job="grafana-stack", instance=~".*mimir.*"} == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "Mimir is down"
          description: "Mimir instance is not responding"

      - alert: LokiDown
        expr: up{job="grafana-stack", instance=~".*loki.*"} == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "Loki is down"
          description: "Loki instance is not responding"

      - alert: TempoDown
        expr: up{job="grafana-stack", instance=~".*tempo.*"} == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "Tempo is down"
          description: "Tempo instance is not responding"

      - alert: AlertmanagerDown
        expr: up{job="alertmanager"} == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "AlertManager is down"
          description: "AlertManager instance is not responding"

    - name: alertmanager.rules
      rules:
      - alert: AlertmanagerConfigurationReloadFailure
        expr: alertmanager_config_last_reload_successful != 1
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "AlertManager configuration reload failure"
          description: "AlertManager configuration reload has failed"

      - alert: AlertmanagerNotificationsFailing
        expr: rate(alertmanager_notifications_failed_total[5m]) > 0
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "AlertManager notifications are failing"
          description: "AlertManager is failing to send notifications"
